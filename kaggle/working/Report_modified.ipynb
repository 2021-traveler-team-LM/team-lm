{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fed85602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "import csv\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "train = pd.read_csv(\"train_2021.csv\", dtype={\"zip_code\" : object})\n",
    "test = pd.read_csv(\"test_2021.csv\", dtype={\"zip_code\" : object})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "99dcbddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fraud\n",
    "train = train[train.fraud != -1]\n",
    "\n",
    "## annual_income\n",
    "train.loc[train.annual_income==-1, 'annual_income'] = np.nan\n",
    "test.loc[test.annual_income==-1, 'annual_income'] = np.nan\n",
    "\n",
    "## age_of_driver\n",
    "train.loc[train.age_of_driver>100, 'age_of_driver'] = np.nan\n",
    "test.loc[test.age_of_driver>100, 'age_of_driver'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e6a7fa",
   "metadata": {},
   "source": [
    "## Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d9db707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set claim_number as index:\n",
    "train = train.set_index('claim_number')\n",
    "test = test.set_index('claim_number')\n",
    "\n",
    "\n",
    "train[\"marital_status\"] = pd.Categorical(train[\"marital_status\"])\n",
    "train[\"high_education_ind\"] = pd.Categorical(train[\"high_education_ind\"])\n",
    "train[\"address_change_ind\"] = pd.Categorical(train[\"address_change_ind\"])\n",
    "train[\"zip_code\"] = pd.Categorical(train[\"zip_code\"])\n",
    "train[\"witness_present_ind\"] = pd.Categorical(train[\"witness_present_ind\"])\n",
    "train[\"policy_report_filed_ind\"] = pd.Categorical(train[\"policy_report_filed_ind\"])\n",
    "train[\"fraud\"] = pd.Categorical(train[\"fraud\"])\n",
    "train[\"claim_day_of_week\"]=pd.Categorical(train[\"claim_day_of_week\"])\n",
    "\n",
    "test[\"marital_status\"] = pd.Categorical(test[\"marital_status\"])\n",
    "test[\"high_education_ind\"] = pd.Categorical(test[\"high_education_ind\"])\n",
    "test[\"address_change_ind\"] = pd.Categorical(test[\"address_change_ind\"])\n",
    "test[\"zip_code\"] = pd.Categorical(test[\"zip_code\"])\n",
    "test[\"witness_present_ind\"] = pd.Categorical(test[\"witness_present_ind\"])\n",
    "test[\"policy_report_filed_ind\"] = pd.Categorical(test[\"policy_report_filed_ind\"])\n",
    "test[\"claim_day_of_week\"]=pd.Categorical(test[\"claim_day_of_week\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddec320",
   "metadata": {},
   "source": [
    "## Add new features: Lat/Lon/State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d3b52f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"zip_code_database.csv\", newline='') as csvfile:\n",
    "    csv_reader = csv.DictReader(csvfile, delimiter=',')\n",
    "    zip_to_lat = {}\n",
    "    zip_to_lon = {}\n",
    "    zip_to_state = {}\n",
    "    for zip_data in csv_reader:\n",
    "        zip_to_lat[zip_data['zip']] = float(zip_data['latitude'])\n",
    "        zip_to_lon[zip_data['zip']] = float(zip_data['longitude'])\n",
    "        zip_to_state[zip_data['zip']] = zip_data['state']\n",
    "        \n",
    "### assuming the '0' zip code is NaN (no such thing as a zip code of 0)   \n",
    "zip_to_lat[np.nan] = np.nan\n",
    "zip_to_lon[np.nan] = np.nan\n",
    "zip_to_state[np.nan] = np.nan\n",
    "\n",
    "zip_to_lat['0'] = np.nan\n",
    "zip_to_lon['0'] = np.nan\n",
    "zip_to_state['0'] = np.nan\n",
    "\n",
    "### transform zip code to latitude, longitude, and state\n",
    "latitude_train = train['zip_code'].apply(\n",
    "    lambda x: zip_to_lat[x]\n",
    ")\n",
    "longitude_train = train['zip_code'].apply(\n",
    "    lambda x: zip_to_lon[x]\n",
    ")\n",
    "state_train = train['zip_code'].apply(\n",
    "    lambda x: zip_to_state[x]\n",
    ")\n",
    "latitude_train.name = 'latitude'\n",
    "longitude_train.name = 'longitude'\n",
    "state_train.name = 'state'\n",
    "\n",
    "latitude_test = test['zip_code'].apply(\n",
    "    lambda x: zip_to_lat[x]\n",
    ")\n",
    "longitude_test = test['zip_code'].apply(\n",
    "    lambda x: zip_to_lon[x]\n",
    ")\n",
    "state_test = test['zip_code'].apply(\n",
    "    lambda x: zip_to_state[x]\n",
    ")\n",
    "latitude_test.name = 'latitude'\n",
    "longitude_test.name = 'longitude'\n",
    "state_test.name = 'state'\n",
    "\n",
    "\n",
    "### Add these new features to the data frame\n",
    "train = pd.concat([train, latitude_train], axis=1)\n",
    "train = pd.concat([train, longitude_train], axis=1)\n",
    "train = pd.concat([train, state_train], axis=1)\n",
    "train[\"state\"] = pd.Categorical(train[\"state\"])\n",
    "\n",
    "test = pd.concat([test, latitude_test], axis=1)\n",
    "test = pd.concat([test, longitude_test], axis=1)\n",
    "test = pd.concat([test, state_test], axis=1)\n",
    "test[\"state\"] = pd.Categorical(test[\"state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f262b2",
   "metadata": {},
   "source": [
    "## Imputation of Missing Values \n",
    "(Use mode and mean to impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bdd6f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of age_of_driver\n",
    "age_of_driver_mean = train.age_of_driver.mean()\n",
    "train['age_of_driver'].fillna(age_of_driver_mean, inplace=True)\n",
    "test['age_of_driver'].fillna(age_of_driver_mean, inplace=True)\n",
    "\n",
    "# mode of marital_status\n",
    "marital_status_mode = train.marital_status.mode().values[0]\n",
    "train['marital_status'].fillna(marital_status_mode, inplace=True)\n",
    "test['marital_status'].fillna(marital_status_mode, inplace=True)\n",
    "\n",
    "# average of annual_income\n",
    "annual_income_mean = train.annual_income.mean()\n",
    "train['annual_income'].fillna(annual_income_mean, inplace=True)\n",
    "test['annual_income'].fillna(annual_income_mean, inplace=True)\n",
    "\n",
    "# mode of witness_present_ind\n",
    "witness_present_mode = train.witness_present_ind.mode().values[0]\n",
    "train['witness_present_ind'].fillna(witness_present_mode, inplace=True)\n",
    "test['witness_present_ind'].fillna(witness_present_mode, inplace=True)\n",
    "\n",
    "# mean of claim_est_payout\n",
    "claim_est_payout_mean = train.claim_est_payout.mean()\n",
    "train['claim_est_payout'].fillna(claim_est_payout_mean, inplace=True)\n",
    "test['claim_est_payout'].fillna(claim_est_payout_mean, inplace=True)\n",
    "\n",
    "# mean of age_of_vehicle\n",
    "age_of_vehicle_mean = train.age_of_vehicle.mean()\n",
    "train['age_of_vehicle'].fillna(age_of_vehicle_mean, inplace=True)\n",
    "test['age_of_vehicle'].fillna(age_of_vehicle_mean, inplace=True)\n",
    "\n",
    "# mean latitude\n",
    "latitude_mean = train.latitude.mean()\n",
    "train['latitude'].fillna(latitude_mean, inplace=True)\n",
    "test['latitude'].fillna(latitude_mean, inplace=True)\n",
    "\n",
    "# mean longitude\n",
    "longitude_mean = train.longitude.mean()\n",
    "train['longitude'].fillna(longitude_mean, inplace=True)\n",
    "test['longitude'].fillna(longitude_mean, inplace=True)\n",
    "\n",
    "# mode of state\n",
    "state_mode = train.state.mode().values[0]\n",
    "train['state'].fillna(state_mode, inplace=True)\n",
    "test['state'].fillna(state_mode, inplace=True)\n",
    "\n",
    "# # print the list of missing columns\n",
    "# print(list(itertools.compress(list(train), list(train.isna().any()))))\n",
    "# print(list(itertools.compress(list(test), list(test.isna().any()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fada93",
   "metadata": {},
   "source": [
    "## One-Hot Encoding of Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fcdb8578",
   "metadata": {},
   "outputs": [],
   "source": [
    "###encoding for TRAIN data set\n",
    "\n",
    "# one-hot encoding for day of week\n",
    "day_dummies = pd.get_dummies(train['claim_day_of_week'], \n",
    "                             prefix='claim_day', drop_first=True)\n",
    "train = pd.concat([train, day_dummies], axis=1)\n",
    "train.drop([\"claim_day_of_week\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for site of accident\n",
    "accident_dummies = pd.get_dummies(train['accident_site'], \n",
    "                                  prefix='accident_site', drop_first=True)\n",
    "train = pd.concat([train, accident_dummies], axis=1)\n",
    "train.drop([\"accident_site\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for channel\n",
    "channel_dummies = pd.get_dummies(train['channel'], \n",
    "                                 prefix='channel', drop_first=True)\n",
    "train = pd.concat([train, channel_dummies], axis=1)\n",
    "train.drop([\"channel\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for vehicle category\n",
    "vehicle_cat_dummies = pd.get_dummies(train['vehicle_category'], \n",
    "                                 prefix='vehicle_category', drop_first=True)\n",
    "train = pd.concat([train, vehicle_cat_dummies], axis=1)\n",
    "train.drop([\"vehicle_category\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for vehicle color\n",
    "vehicle_color_dummies = pd.get_dummies(train['vehicle_color'], \n",
    "                                 prefix='vehicle_color', drop_first=True)\n",
    "train = pd.concat([train, vehicle_color_dummies], axis=1)\n",
    "train.drop([\"vehicle_color\"], axis=1, inplace=True)\n",
    "\n",
    "# # one-hot encoding for claim month\n",
    "# vehicle_color_dummies = pd.get_dummies(train['claim_month'], \n",
    "#                                  prefix='claim_month', drop_first=True)\n",
    "# train = pd.concat([train, vehicle_color_dummies], axis=1)\n",
    "# train.drop([\"claim_month\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for state\n",
    "state_dummies = pd.get_dummies(train['state'],\n",
    "                               prefix='state', drop_first=True)\n",
    "train = pd.concat([train, state_dummies], axis=1)\n",
    "train.drop([\"state\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "### encoding for TEST data set\n",
    "\n",
    "# one-hot encoding for day of week\n",
    "day_dummies = pd.get_dummies(test[\"claim_day_of_week\"], \n",
    "                             prefix='claim_day', drop_first=True)\n",
    "test = pd.concat([test, day_dummies], axis=1)\n",
    "test.drop([\"claim_day_of_week\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for site of accident\n",
    "accident_dummies = pd.get_dummies(test['accident_site'], \n",
    "                                  prefix='accident_site', drop_first=True)\n",
    "test = pd.concat([test, accident_dummies], axis=1)\n",
    "test.drop([\"accident_site\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for channel\n",
    "channel_dummies = pd.get_dummies(test['channel'], \n",
    "                                 prefix='channel', drop_first=True)\n",
    "test = pd.concat([test, channel_dummies], axis=1)\n",
    "test.drop([\"channel\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for vehicle category\n",
    "vehicle_cat_dummies = pd.get_dummies(test['vehicle_category'], \n",
    "                                 prefix='vehicle_category', drop_first=True)\n",
    "test = pd.concat([test, vehicle_cat_dummies], axis=1)\n",
    "test.drop([\"vehicle_category\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for vehicle color\n",
    "vehicle_color_dummies = pd.get_dummies(test['vehicle_color'], \n",
    "                                 prefix='vehicle_color', drop_first=True)\n",
    "test = pd.concat([test, vehicle_color_dummies], axis=1)\n",
    "test.drop([\"vehicle_color\"], axis=1, inplace=True)\n",
    "\n",
    "# # one-hot encoding for claim month\n",
    "# vehicle_color_dummies = pd.get_dummies(test['claim_month'], \n",
    "#                                  prefix='claim_month', drop_first=True)\n",
    "# test = pd.concat([test, vehicle_color_dummies], axis=1)\n",
    "# test.drop([\"claim_month\"], axis=1, inplace=True)\n",
    "\n",
    "# one-hot encoding for state\n",
    "state_dummies = pd.get_dummies(test['state'],\n",
    "                               prefix='state', drop_first=True)\n",
    "test = pd.concat([test, state_dummies], axis=1)\n",
    "test.drop([\"state\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ccbba80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean up variable names by making them all lowercase with underscore separators.\n",
    "train.columns = map(\n",
    "    lambda s: s.lower().replace(' ', '_'), \n",
    "    train.columns)\n",
    "\n",
    "test.columns = map(\n",
    "    lambda s: s.lower().replace(' ', '_'), \n",
    "    test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fabe1f",
   "metadata": {},
   "source": [
    "## add grouped-by means as the new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b5ca0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"fraud\"] = train[\"fraud\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74717fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## gender\n",
    "# grouped_gender = train[\"fraud_1\"].groupby(train['gender'])\n",
    "# grouped_gender_mean = grouped_gender.mean().to_frame()\n",
    "# grouped_gender_mean['gender']=grouped_gender_mean.index\n",
    "# grouped_gender_mean['fraud_gender'] = grouped_gender_mean['fraud']\n",
    "# grouped_gender_mean.drop('fraud', axis = 1, inplace = True)\n",
    "# grouped_gender_mean=grouped_gender_mean.drop(['gender'],axis=1)\n",
    "# train = pd.merge(train, grouped_gender_mean, on = \"gender\", how = \"left\")\n",
    "# test = pd.merge(test, grouped_gender_mean, on = \"gender\", how = \"left\")\n",
    "# grouped_gender_mean\n",
    "\n",
    "# ## marital_status\n",
    "# grouped_marital_status = train[\"fraud\"].groupby(train['marital_status'])\n",
    "# grouped_marital_status_mean = grouped_marital_status.mean().to_frame()\n",
    "# grouped_marital_status_mean['marital_status']=grouped_marital_status_mean.index\n",
    "# grouped_marital_status_mean['fraud_marital_status'] = grouped_marital_status_mean['fraud']\n",
    "# grouped_marital_status_mean.drop('fraud', axis = 1, inplace = True)\n",
    "# grouped_marital_status_mean=grouped_marital_status_mean.drop(['marital_status'],axis=1)\n",
    "# train = pd.merge(train, grouped_marital_status_mean, on = \"marital_status\", how = \"left\")\n",
    "# test = pd.merge(test, grouped_marital_status_mean, on = \"marital_status\", how = \"left\")\n",
    "# grouped_marital_status_mean\n",
    "\n",
    "# ## high_education_ind\n",
    "# grouped_high_education_ind = train[\"fraud\"].groupby(train['high_education_ind'])\n",
    "# grouped_high_education_ind_mean = grouped_high_education_ind.mean().to_frame()\n",
    "# grouped_high_education_ind_mean['high_education_ind']=grouped_high_education_ind_mean.index\n",
    "# grouped_high_education_ind_mean['fraud_high_education_ind'] = grouped_high_education_ind_mean['fraud']\n",
    "# grouped_high_education_ind_mean.drop('fraud', axis = 1, inplace = True)\n",
    "# grouped_high_education_ind_mean=grouped_high_education_ind_mean.drop(['high_education_ind'],axis=1)\n",
    "# train = pd.merge(train, grouped_high_education_ind_mean, on = \"high_education_ind\", how = \"left\")\n",
    "# test = pd.merge(test, grouped_high_education_ind_mean, on = \"high_education_ind\", how = \"left\")\n",
    "# grouped_high_education_ind_mean\n",
    "\n",
    "\n",
    "# ## address_change_ind\n",
    "# grouped_address_change_ind = train[\"fraud\"].groupby(train['address_change_ind'])\n",
    "# grouped_address_change_ind_mean = grouped_address_change_ind.mean().to_frame()\n",
    "# grouped_address_change_ind_mean['address_change_ind']=grouped_address_change_ind_mean.index\n",
    "# grouped_address_change_ind_mean['fraud_address_change_ind'] = grouped_address_change_ind_mean['fraud']\n",
    "# grouped_address_change_ind_mean.drop('fraud', axis = 1, inplace = True)\n",
    "# grouped_address_change_ind_mean=grouped_address_change_ind_mean.drop(['address_change_ind'],axis=1)\n",
    "# train = pd.merge(train, grouped_address_change_ind_mean, on = \"address_change_ind\", how = \"left\")\n",
    "# test = pd.merge(test, grouped_address_change_ind_mean, on = \"address_change_ind\", how = \"left\")\n",
    "# grouped_address_change_ind_mean\n",
    "\n",
    "# ## living_status\n",
    "# grouped_living_status = train[\"fraud\"].groupby(train['living_status'])\n",
    "# grouped_living_status_mean = grouped_living_status.mean().to_frame()\n",
    "# grouped_living_status_mean['living_status']=grouped_living_status_mean.index\n",
    "# grouped_living_status_mean['fraud_living_status'] = grouped_living_status_mean['fraud']\n",
    "# grouped_living_status_mean.drop('fraud', axis = 1, inplace = True)\n",
    "# grouped_living_status_mean=grouped_living_status_mean.drop(['living_status'],axis=1)\n",
    "# train = pd.merge(train, grouped_living_status_mean, on = \"living_status\", how = \"left\")\n",
    "# test = pd.merge(test, grouped_living_status_mean, on = \"living_status\", how = \"left\")\n",
    "# grouped_living_status_mean\n",
    "\n",
    "# ## zip_code\n",
    "# grouped_zip_code = train[\"fraud\"].groupby(train['zip_code'])\n",
    "# grouped_zip_code_mean = grouped_zip_code.mean().to_frame()\n",
    "# grouped_zip_code_mean['zip_code']=grouped_zip_code_mean.index\n",
    "# grouped_zip_code_mean['fraud_zip_code'] = grouped_zip_code_mean['fraud']\n",
    "# grouped_zip_code_mean.drop('fraud', axis = 1, inplace = True)\n",
    "# grouped_zip_code_mean=grouped_zip_code_mean.drop(['zip_code'],axis=1)\n",
    "# train = pd.merge(train, grouped_zip_code_mean, on = \"zip_code\", how = \"left\")\n",
    "# test = pd.merge(test, grouped_zip_code_mean, on = \"zip_code\", how = \"left\")\n",
    "# grouped_zip_code_mean\n",
    "\n",
    "# ## claim_date\n",
    "# grouped_claim_date = train[\"fraud\"].groupby(train['claim_date'])\n",
    "# grouped_claim_date_mean = grouped_claim_date.mean().to_frame()\n",
    "# grouped_claim_date_mean['claim_date']=grouped_claim_date_mean.index\n",
    "# grouped_claim_date_mean['fraud_claim_date'] = grouped_claim_date_mean['fraud']\n",
    "# grouped_claim_date_mean.drop('fraud', axis = 1, inplace = True)\n",
    "# grouped_claim_date_mean=grouped_claim_date_mean.drop(['claim_date'],axis=1)\n",
    "# train = pd.merge(train, grouped_claim_date_mean, on = \"claim_date\", how = \"left\")\n",
    "# test = pd.merge(test, grouped_claim_date_mean, on = \"claim_date\", how = \"left\")\n",
    "# grouped_claim_date_mean\n",
    "\n",
    "# ## witness_present_ind\n",
    "# grouped_witness_present_ind = train[\"fraud\"].groupby(train['witness_present_ind'])\n",
    "# grouped_witness_present_ind_mean = grouped_witness_present_ind.mean().to_frame()\n",
    "# grouped_witness_present_ind_mean['witness_present_ind']=grouped_witness_present_ind_mean.index\n",
    "# grouped_witness_present_ind_mean['fraud_witness_present_ind'] = grouped_witness_present_ind_mean['fraud']\n",
    "# grouped_witness_present_ind_mean.drop('fraud', axis = 1, inplace = True)\n",
    "# grouped_witness_present_ind_mean=grouped_witness_present_ind_mean.drop(['witness_present_ind'],axis=1)\n",
    "# train = pd.merge(train, grouped_witness_present_ind_mean, on = \"witness_present_ind\", how = \"left\")\n",
    "# test = pd.merge(test, grouped_witness_present_ind_mean, on = \"witness_present_ind\", how = \"left\")\n",
    "# grouped_witness_present_ind_mean\n",
    "\n",
    "# ## policy_report_filed_ind\n",
    "# grouped_policy_report_filed_ind = train[\"fraud\"].groupby(train['policy_report_filed_ind'])\n",
    "# grouped_policy_report_filed_ind_mean = grouped_policy_report_filed_ind.mean().to_frame()\n",
    "# grouped_policy_report_filed_ind_mean['policy_report_filed_ind']=grouped_policy_report_filed_ind_mean.index\n",
    "# grouped_policy_report_filed_ind_mean['fraud_policy_report_filed_ind'] = grouped_policy_report_filed_ind_mean['fraud']\n",
    "# grouped_policy_report_filed_ind_mean.drop('fraud', axis = 1, inplace = True)\n",
    "# grouped_policy_report_filed_ind_mean=grouped_policy_report_filed_ind_mean.drop(['policy_report_filed_ind'],axis=1)\n",
    "# train = pd.merge(train, grouped_policy_report_filed_ind_mean, on = \"policy_report_filed_ind\", how = \"left\")\n",
    "# test = pd.merge(test, grouped_policy_report_filed_ind_mean, on = \"policy_report_filed_ind\", how = \"left\")\n",
    "# grouped_policy_report_filed_ind_mean\n",
    "\n",
    "# ## state\n",
    "# grouped_state = train[\"fraud\"].groupby(train['state'])\n",
    "# grouped_state_mean = grouped_state.mean().to_frame()\n",
    "# grouped_state_mean['state']=grouped_state_mean.index\n",
    "# grouped_state_mean['fraud_state'] = grouped_state_mean['fraud']\n",
    "# grouped_state_mean.drop('fraud', axis = 1, inplace = True)\n",
    "# grouped_state_mean=grouped_state_mean.drop(['state'],axis=1)\n",
    "# train = pd.merge(train, grouped_state_mean, on = \"state\", how = \"left\")\n",
    "# test = pd.merge(test, grouped_state_mean, on = \"state\", how = \"left\")\n",
    "# grouped_state_mean\n",
    "\n",
    "# ## accident_site\n",
    "# grouped_accident_site = raw_train[\"fraud\"].groupby(raw_train['accident_site'])\n",
    "# grouped_accident_site_mean = grouped_accident_site.mean().to_frame()\n",
    "# grouped_accident_site_mean['accident_site']=grouped_accident_site_mean.index\n",
    "# grouped_accident_site_mean['fraud_accident_site'] = grouped_accident_site_mean['fraud']\n",
    "# grouped_accident_site_mean.drop('fraud', axis = 1, inplace = True)\n",
    "# grouped_accident_site_mean=grouped_accident_site_mean.drop(['accident_site'],axis=1)\n",
    "# raw_train = pd.merge(raw_train, grouped_accident_site_mean, on = \"accident_site\", how = \"left\")\n",
    "# train['fraud_accident_site'] = raw_train['fraud_accident_site']\n",
    "# raw_test = pd.merge(raw_test, grouped_accident_site_mean, on = \"accident_site\", how = \"left\")\n",
    "# test['fraud_accident_site'] = raw_test['fraud_accident_site']\n",
    "# grouped_accident_site_mean\n",
    "\n",
    "# ## channel\n",
    "# grouped_channel = raw_train[\"fraud\"].groupby(raw_train['channel'])\n",
    "# grouped_channel_mean = grouped_channel.mean().to_frame()\n",
    "# grouped_channel_mean['channel']=grouped_channel_mean.index\n",
    "# grouped_channel_mean['fraud_channel'] = grouped_channel_mean['fraud']\n",
    "# grouped_channel_mean.drop('fraud', axis = 1, inplace = True)\n",
    "# grouped_channel_mean=grouped_channel_mean.drop(['channel'],axis=1)\n",
    "# raw_train = pd.merge(raw_train, grouped_channel_mean, on = \"channel\", how = \"left\")\n",
    "# train['fraud_channel'] = raw_train['fraud_channel']\n",
    "# raw_test = pd.merge(raw_test, grouped_channel_mean, on = \"channel\", how = \"left\")\n",
    "# test['fraud_channel'] = raw_test['fraud_channel']\n",
    "              \n",
    "# grouped_channel_mean\n",
    "\n",
    "# ## vehicle_category\n",
    "# grouped_vehicle_category = raw_train[\"fraud\"].groupby(raw_train['vehicle_category'])\n",
    "# grouped_vehicle_category_mean = grouped_vehicle_category.mean().to_frame()\n",
    "# grouped_vehicle_category_mean['vehicle_category']=grouped_vehicle_category_mean.index\n",
    "# grouped_vehicle_category_mean['fraud_vehicle_category'] = grouped_vehicle_category_mean['fraud']\n",
    "# grouped_vehicle_category_mean.drop('fraud', axis = 1, inplace = True)\n",
    "# grouped_vehicle_category_mean=grouped_vehicle_category_mean.drop(['vehicle_category'],axis=1)\n",
    "# raw_train = pd.merge(raw_train, grouped_vehicle_category_mean, on = \"vehicle_category\", how = \"left\")\n",
    "# train['fraud_vehicle_category'] = raw_train['fraud_vehicle_category']\n",
    "# raw_test = pd.merge(raw_test, grouped_vehicle_category_mean, on = \"vehicle_category\", how = \"left\")\n",
    "# test['fraud_vehicle_category'] = raw_test['fraud_vehicle_category']              \n",
    "# grouped_vehicle_category_mean\n",
    "\n",
    "# ## vehicle_color\n",
    "# grouped_vehicle_color = raw_train[\"fraud\"].groupby(raw_train['vehicle_color'])\n",
    "# grouped_vehicle_color_mean = grouped_vehicle_color.mean().to_frame()\n",
    "# grouped_vehicle_color_mean['vehicle_color']=grouped_vehicle_color_mean.index\n",
    "# grouped_vehicle_color_mean['fraud_vehicle_color'] = grouped_vehicle_color_mean['fraud']\n",
    "# grouped_vehicle_color_mean.drop('fraud', axis = 1, inplace = True)\n",
    "# grouped_vehicle_color_mean=grouped_vehicle_color_mean.drop(['vehicle_color'],axis=1)\n",
    "# raw_train = pd.merge(raw_train, grouped_vehicle_color_mean, on = \"vehicle_color\", how = \"left\")\n",
    "# train['fraud_vehicle_color'] = raw_train['fraud_vehicle_color']\n",
    "# raw_test = pd.merge(raw_test, grouped_vehicle_color_mean, on = \"vehicle_color\", how = \"left\")\n",
    "# test['fraud_vehicle_color'] = raw_test['fraud_vehicle_color']              \n",
    "# grouped_vehicle_color_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9aff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Encode gender and living status and state  #####\n",
    "train[\"living_status\"] = pd.Categorical(train[\"living_status\"])\n",
    "train[\"gender\"] = np.where(train[\"gender\"].str.contains(\"M\"), 1, 0)\n",
    "train[\"living_status\"] = np.where(train[\"living_status\"].str.contains(\"Rent\"), 1, 0)\n",
    "\n",
    "test[\"living_status\"] = pd.Categorical(test[\"living_status\"])\n",
    "test[\"gender\"] = np.where(test[\"gender\"].str.contains(\"M\"), 1, 0)\n",
    "test[\"living_status\"] = np.where(test[\"living_status\"].str.contains(\"Rent\"), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "166d77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train_data_cleaned.csv\")\n",
    "test.to_csv(\"test_data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "65c84d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_of_driver</th>\n",
       "      <th>gender</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>safty_rating</th>\n",
       "      <th>annual_income</th>\n",
       "      <th>high_education_ind</th>\n",
       "      <th>address_change_ind</th>\n",
       "      <th>living_status</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>claim_date</th>\n",
       "      <th>past_num_of_claims</th>\n",
       "      <th>witness_present_ind</th>\n",
       "      <th>liab_prct</th>\n",
       "      <th>policy_report_filed_ind</th>\n",
       "      <th>claim_est_payout</th>\n",
       "      <th>age_of_vehicle</th>\n",
       "      <th>vehicle_price</th>\n",
       "      <th>vehicle_weight</th>\n",
       "      <th>fraud</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>claim_day_monday</th>\n",
       "      <th>claim_day_saturday</th>\n",
       "      <th>claim_day_sunday</th>\n",
       "      <th>claim_day_thursday</th>\n",
       "      <th>claim_day_tuesday</th>\n",
       "      <th>claim_day_wednesday</th>\n",
       "      <th>accident_site_local</th>\n",
       "      <th>accident_site_parking_lot</th>\n",
       "      <th>channel_online</th>\n",
       "      <th>channel_phone</th>\n",
       "      <th>vehicle_category_large</th>\n",
       "      <th>vehicle_category_medium</th>\n",
       "      <th>vehicle_color_blue</th>\n",
       "      <th>vehicle_color_gray</th>\n",
       "      <th>vehicle_color_other</th>\n",
       "      <th>vehicle_color_red</th>\n",
       "      <th>vehicle_color_silver</th>\n",
       "      <th>vehicle_color_white</th>\n",
       "      <th>state_co</th>\n",
       "      <th>state_ia</th>\n",
       "      <th>state_pa</th>\n",
       "      <th>state_va</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim_number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85</td>\n",
       "      <td>38301.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>80006</td>\n",
       "      <td>12/16/2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>7530.940993</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12885.45235</td>\n",
       "      <td>16161.33381</td>\n",
       "      <td>0</td>\n",
       "      <td>39.82</td>\n",
       "      <td>-105.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>30445.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15021</td>\n",
       "      <td>2/12/2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>2966.024895</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29429.45218</td>\n",
       "      <td>28691.96422</td>\n",
       "      <td>0</td>\n",
       "      <td>40.38</td>\n",
       "      <td>-80.39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87</td>\n",
       "      <td>38923.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20158</td>\n",
       "      <td>12/6/2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6283.888333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21701.18195</td>\n",
       "      <td>22090.94758</td>\n",
       "      <td>1</td>\n",
       "      <td>39.13</td>\n",
       "      <td>-77.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58</td>\n",
       "      <td>40605.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15024</td>\n",
       "      <td>5/5/2016</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>6169.747994</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13198.27344</td>\n",
       "      <td>38329.58106</td>\n",
       "      <td>1</td>\n",
       "      <td>40.54</td>\n",
       "      <td>-79.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95</td>\n",
       "      <td>36380.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50034</td>\n",
       "      <td>10/27/2015</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4541.387150</td>\n",
       "      <td>7.0</td>\n",
       "      <td>38060.21122</td>\n",
       "      <td>25876.56319</td>\n",
       "      <td>0</td>\n",
       "      <td>42.47</td>\n",
       "      <td>-93.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age_of_driver  gender marital_status  safty_rating  \\\n",
       "claim_number                                                       \n",
       "1                      46.0       1            1.0            85   \n",
       "3                      21.0       0            0.0            75   \n",
       "4                      49.0       0            0.0            87   \n",
       "5                      58.0       0            1.0            58   \n",
       "6                      38.0       1            1.0            95   \n",
       "\n",
       "              annual_income high_education_ind address_change_ind  \\\n",
       "claim_number                                                        \n",
       "1                   38301.0                  1                  1   \n",
       "3                   30445.0                  0                  1   \n",
       "4                   38923.0                  0                  1   \n",
       "5                   40605.0                  1                  0   \n",
       "6                   36380.0                  1                  0   \n",
       "\n",
       "             living_status zip_code  claim_date  past_num_of_claims  \\\n",
       "claim_number                                                          \n",
       "1                        1    80006  12/16/2016                   1   \n",
       "3                        1    15021   2/12/2015                   1   \n",
       "4                        0    20158   12/6/2016                   0   \n",
       "5                        0    15024    5/5/2016                   3   \n",
       "6                        1    50034  10/27/2015                   0   \n",
       "\n",
       "             witness_present_ind  liab_prct policy_report_filed_ind  \\\n",
       "claim_number                                                          \n",
       "1                            0.0         74                       0   \n",
       "3                            1.0         79                       0   \n",
       "4                            0.0          0                       0   \n",
       "5                            0.0         99                       1   \n",
       "6                            1.0          7                       0   \n",
       "\n",
       "              claim_est_payout  age_of_vehicle  vehicle_price  vehicle_weight  \\\n",
       "claim_number                                                                    \n",
       "1                  7530.940993             9.0    12885.45235     16161.33381   \n",
       "3                  2966.024895             4.0    29429.45218     28691.96422   \n",
       "4                  6283.888333             3.0    21701.18195     22090.94758   \n",
       "5                  6169.747994             4.0    13198.27344     38329.58106   \n",
       "6                  4541.387150             7.0    38060.21122     25876.56319   \n",
       "\n",
       "             fraud  latitude  longitude  claim_day_monday  claim_day_saturday  \\\n",
       "claim_number                                                                    \n",
       "1                0     39.82    -105.10                 0                   0   \n",
       "3                0     40.38     -80.39                 0                   0   \n",
       "4                1     39.13     -77.66                 0                   0   \n",
       "5                1     40.54     -79.80                 0                   0   \n",
       "6                0     42.47     -93.64                 0                   0   \n",
       "\n",
       "              claim_day_sunday  claim_day_thursday  claim_day_tuesday  \\\n",
       "claim_number                                                            \n",
       "1                            0                   0                  0   \n",
       "3                            0                   1                  0   \n",
       "4                            0                   0                  1   \n",
       "5                            0                   1                  0   \n",
       "6                            0                   0                  1   \n",
       "\n",
       "              claim_day_wednesday  accident_site_local  \\\n",
       "claim_number                                             \n",
       "1                               0                    1   \n",
       "3                               0                    0   \n",
       "4                               0                    1   \n",
       "5                               0                    1   \n",
       "6                               0                    0   \n",
       "\n",
       "              accident_site_parking_lot  channel_online  channel_phone  \\\n",
       "claim_number                                                             \n",
       "1                                     0               0              0   \n",
       "3                                     0               1              0   \n",
       "4                                     0               0              0   \n",
       "5                                     0               0              0   \n",
       "6                                     0               0              0   \n",
       "\n",
       "              vehicle_category_large  vehicle_category_medium  \\\n",
       "claim_number                                                    \n",
       "1                                  0                        0   \n",
       "3                                  1                        0   \n",
       "4                                  0                        0   \n",
       "5                                  0                        1   \n",
       "6                                  0                        1   \n",
       "\n",
       "              vehicle_color_blue  vehicle_color_gray  vehicle_color_other  \\\n",
       "claim_number                                                                \n",
       "1                              0                   0                    0   \n",
       "3                              0                   0                    0   \n",
       "4                              0                   0                    0   \n",
       "5                              0                   0                    1   \n",
       "6                              0                   1                    0   \n",
       "\n",
       "              vehicle_color_red  vehicle_color_silver  vehicle_color_white  \\\n",
       "claim_number                                                                 \n",
       "1                             0                     0                    1   \n",
       "3                             0                     0                    1   \n",
       "4                             0                     0                    1   \n",
       "5                             0                     0                    0   \n",
       "6                             0                     0                    0   \n",
       "\n",
       "              state_co  state_ia  state_pa  state_va  \n",
       "claim_number                                          \n",
       "1                    1         0         0         0  \n",
       "3                    0         0         1         0  \n",
       "4                    0         0         0         1  \n",
       "5                    0         0         1         0  \n",
       "6                    0         1         0         0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e5c0ce",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cc32837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop month, day and year data, drop vehicle color, zipcode, claim_date, claim_number and SP_Index  #####\n",
    "train.drop([   \"claim_day_monday\", \"claim_day_tuesday\", \"claim_day_wednesday\", \"claim_day_thursday\", \n",
    "               \"claim_day_saturday\", \"claim_day_sunday\",  \"zip_code\", \"claim_date\",   \"vehicle_color_blue\", \n",
    "               \"vehicle_color_gray\", \"vehicle_color_other\", \"vehicle_color_red\", \n",
    "              \"vehicle_color_silver\", \"vehicle_color_white\"], axis =1, inplace=True)\n",
    "\n",
    "test.drop([\"claim_day_monday\", \"claim_day_tuesday\", \"claim_day_wednesday\", \"claim_day_thursday\", \n",
    "               \"claim_day_saturday\", \"claim_day_sunday\", \"zip_code\", \"claim_date\", \"vehicle_color_blue\", \n",
    "               \"vehicle_color_gray\", \"vehicle_color_other\", \"vehicle_color_red\", \n",
    "              \"vehicle_color_silver\", \"vehicle_color_white\"], axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b77c1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.filter(regex=\"^(?!state_).*$\")\n",
    "test = test.filter(regex=\"^(?!state_).*$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b9f68a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "y = train[\"fraud\"]\n",
    "X = train.drop(\"fraud\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "eb7d0778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.153834916 (+/- 0.0145)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.metrics import f1_score\n",
    "nb = GaussianNB()\n",
    "scores = model_selection.cross_val_score(nb, X.values, y.values, cv = 5, scoring = 'f1')\n",
    "print(\"F1: %0.9f (+/- %0.4f)\" % (scores.mean(), scores.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
